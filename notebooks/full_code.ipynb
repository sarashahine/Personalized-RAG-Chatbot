{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Update' from 'telegram' (c:\\Users\\user\\Documents\\GitHub\\Personalized-RAG-Chatbot\\.venv\\lib\\site-packages\\telegram\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtelegram\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Update\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtelegram\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Application, CommandHandler, MessageHandler, ContextTypes, filters\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Update' from 'telegram' (c:\\Users\\user\\Documents\\GitHub\\Personalized-RAG-Chatbot\\.venv\\lib\\site-packages\\telegram\\__init__.py)"
     ]
    }
   ],
   "source": [
    "%pip install python-telegram-bot --quiet\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "from telegram import Update\n",
    "from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters\n",
    "from dotenv import load_dotenv\n",
    "from elevenlabs.client import ElevenLabs\n",
    "import openai\n",
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "from shared_redis import r, RedisHistoryManager, format_history_for_prompt\n",
    "from io import BytesIO\n",
    "from functools import partial\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "\n",
    "# Load APIs and data\n",
    "load_dotenv()\n",
    "elevenlabs = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "telegram_token = os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "\n",
    "index = faiss.read_index(\"storage/openai_index.faiss\")\n",
    "\n",
    "with open(\"storage/chunks_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "history = RedisHistoryManager(max_messages=40)\n",
    "\n",
    "\n",
    "# Functions\n",
    "def get_embedding(text: str, model=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    embedding = response.data[0].embedding\n",
    "    return np.array(embedding, dtype='float32')\n",
    "\n",
    "def search_index(query, k=5, min_score=0.4):\n",
    "\n",
    "    query_vector = get_embedding(query).reshape(1, -1)\n",
    "    query_vector /= np.linalg.norm(query_vector)\n",
    "\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "\n",
    "    # Build results, but ensure at least one item is returned\n",
    "    results = []\n",
    "    top_pairs = list(zip(distances[0], indices[0]))\n",
    "\n",
    "    for dist, idx in top_pairs:\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        if dist < min_score:\n",
    "            continue  # skip low scores\n",
    "        chunk_data = metadata[idx]\n",
    "        results.append({\n",
    "            \"score\": float(dist),\n",
    "            \"chunk\": chunk_data[\"content\"],\n",
    "            \"metadata\": {\n",
    "                \"id\": chunk_data[\"id\"],\n",
    "                \"title\": chunk_data.get(\"title\", \"\"),\n",
    "                \"source\": chunk_data.get(\"source\", \"\")\n",
    "            }\n",
    "        })\n",
    "\n",
    "    if results and top_pairs:\n",
    "        dist, idx = top_pairs[0]\n",
    "        if idx >= 0:\n",
    "            chunk_data = metadata[idx]\n",
    "            results.append({\n",
    "                \"score\": float(dist),\n",
    "                \"chunk\": chunk_data[\"content\"],\n",
    "                \"metadata\": {\n",
    "                    \"id\": chunk_data[\"id\"],\n",
    "                    \"title\": chunk_data.get(\"title\", \"\"),\n",
    "                    \"source\": chunk_data.get(\"source\", \"\")\n",
    "                }\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "with open(\"character.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    character = json.load(f)\n",
    "\n",
    "def build_persona_preamble(c) -> str:\n",
    "    if isinstance(c, list):\n",
    "        c = next((x for x in c if isinstance(x, dict) and 'lexicon' in x), (c[0] if c and isinstance(c[0], dict) else {}))\n",
    "    elif not isinstance(c, dict):\n",
    "        c = {}\n",
    "\n",
    "    role_instructions = c.get(\"role_instructions\")\n",
    "    t = c.get(\"tone\", {})\n",
    "    tone = \", \".join(t.values())\n",
    "\n",
    "    lex = c.get(\"lexicon\") or {}\n",
    "    inv = \"\\n- \".join(lex.get(\"invocations\") or [])\n",
    "    honors = \"\\n- \".join(lex.get(\"honorifics\") or [])\n",
    "    ashura = \"\\n- \".join(lex.get(\"ashura_register\") or [])\n",
    "    bins = \"\\n- \".join(lex.get(\"binaries\") or [])\n",
    "    values = \"\\n- \".join(lex.get(\"values\") or [])\n",
    "\n",
    "    dm_formal = \"\\n- \".join(lex.get(\"discourse_markers_formal\") or [])\n",
    "    dm_colloq = \"\\n- \".join(lex.get(\"discourse_markers_colloquial\") or [])\n",
    "    emph = \"\\n- \".join(lex.get(\"emphasis_markers\") or [])\n",
    "    key_terms = \"\\n- \".join(lex.get(\"key_terms\") or [])\n",
    "\n",
    "    reh = c.get(\"rhetorical_scaffold\") or {}\n",
    "    open = \"\\n- \".join(reh.get(\"open\") or [])\n",
    "    develop = \"\\n- \".join(reh.get(\"develop\") or [])\n",
    "    evidence = \"\\n- \".join(reh.get(\"evidence\") or [])\n",
    "    application = \"\\n- \".join(reh.get(\"application\") or [])\n",
    "    closure = \"\\n- \".join(reh.get(\"closure\") or [])\n",
    "\n",
    "    pacing = c.get(\"response_pacing\", {})\n",
    "    response_pacing = \", \".join(pacing.values())\n",
    "\n",
    "    greetings = \"\\n- \".join(c.get(\"greeting_templates\") or [])\n",
    "    closing = \"\\n- \".join(c.get(\"closing_templates\") or [])\n",
    "    condolences = \"\\n- \".join(c.get(\"condolence_templates\") or [])\n",
    "\n",
    "    q = c.get(\"quote_frames\", {})\n",
    "    quote_frames = \", \".join(q.values())\n",
    "\n",
    "    do = \"\\n- \".join(c.get(\"do\") or [])\n",
    "    dont = \"\\n- \".join(c.get(\"dont\") or [])\n",
    "\n",
    "    snippet = c.get(\"style_snippets\", {})\n",
    "    style_snippets = \", \".join(snippet.values())\n",
    "\n",
    "    micro = c.get(\"micro_templates\", {})\n",
    "    micro_templates = \", \".join(micro.values())\n",
    "\n",
    "    topics = \"\\n- \".join(c.get(\"topics\") or [])\n",
    "\n",
    "    tk = c.get(\"topics_knowledge\") or {}\n",
    "    personal_section = \"\"\n",
    "    other_topics_sections = []\n",
    "    if isinstance(tk, dict):\n",
    "        for name, data in tk.items():\n",
    "            is_personal = isinstance(name, str) and \"سيرة السيد هاشم صفيّ الدين\" in name\n",
    "            highlights = []\n",
    "            points = []\n",
    "            use_with = None\n",
    "            if isinstance(data, dict):\n",
    "                highlights = data.get(\"highlights\") or []\n",
    "                points = data.get(\"points\") or []\n",
    "                use_with = data.get(\"use_with\")\n",
    "            if is_personal:\n",
    "                lines = []\n",
    "                if points:\n",
    "                    lines.extend(points)\n",
    "                elif highlights:\n",
    "                    lines.extend(highlights)\n",
    "                else:\n",
    "                    lines.extend([f\"{k}: {v}\" for k, v in data.items()])\n",
    "                personal_section = \"\\n\".join([\"السيرة الشخصية:\"] + [f\"- {x}\" for x in lines])\n",
    "            else:\n",
    "                lines = []\n",
    "                if highlights:\n",
    "                    lines.extend(highlights)\n",
    "                elif points:\n",
    "                    lines.extend(points)\n",
    "                else:\n",
    "                    lines.extend([f\"{k}: {v}\" for k, v in data.items()])\n",
    "                section = \"\\n\".join([name + \":\"] + [f\"- {x}\" for x in lines] + ([f\"- use_with: {use_with}\"] if use_with else []))\n",
    "                other_topics_sections.append(section)\n",
    "    topics_knowledge_personal = personal_section\n",
    "    topics_knowledge_other = \"\\n\\n\".join(other_topics_sections)\n",
    "\n",
    "    cu = c.get(\"contextual_usage\") or {}\n",
    "    contextual_usage = \"\\n\".join([\"قيود الاستخدام السياقي (إلزامي):\"] + [f\"- {k}: {v}\" for k, v in cu.items()])\n",
    "\n",
    "    return (\n",
    "        f\"الشخصية: {c.get('name','')}\\n\"\n",
    "        f\"الغرض: {c.get('purpose','')}\\n\"\n",
    "        f\"تعليمات الدور: {role_instructions}\\n\"\n",
    "        f\"النبرة: {tone}\\n\"\n",
    "        f\"الافتتاحيات:\\n- {inv}\\n\"\n",
    "        f\"الألقاب:\\n- {honors}\\n\"\n",
    "        f\"سجل عاشورائي:\\n- {ashura}\\n\"\n",
    "        f\"الثنائيات:\\n- {bins}\\n\"\n",
    "        f\"القيم:\\n- {values}\\n\"\n",
    "        f\"روابط الخطاب (فصحى):\\n- {dm_formal}\\n\"\n",
    "        f\"روابط الخطاب (عامية):\\n- {dm_colloq}\\n\"\n",
    "        f\"علامات التأكيد:\\n- {emph}\\n\"\n",
    "        f\"مصطلحات مفتاحية:\\n- {key_terms}\\n\"\n",
    "        f\"التمهيد البلاغي:\\n- {open}\\n\"\n",
    "        f\"التطوير البلاغي:\\n- {develop}\\n\"\n",
    "        f\"أمثلة وأدلة:\\n- {evidence}\\n\"\n",
    "        f\"تطبيق البلاغة:\\n- {application}\\n\"\n",
    "        f\"الإغلاق البلاغي:\\n- {closure}\\n\"\n",
    "        f\"إيقاع الاستجابة: {response_pacing}\\n\"\n",
    "        f\"قوالب الترحيب:\\n- {greetings}\\n\"\n",
    "        f\"قوالب الختام:\\n- {closing}\\n\"\n",
    "        f\"قوالب التعزية:\\n- {condolences}\\n\"\n",
    "        f\"أطر الاقتباس: {quote_frames}\\n\"\n",
    "        f\"افعل:\\n- {do}\\n\"\n",
    "        f\"لا تفعل:\\n- {dont}\\n\"\n",
    "        f\"مقتطفات أسلوبية: {style_snippets}\\n\"\n",
    "        f\"قوالب دقيقة: {micro_templates}\\n\"\n",
    "        f\"الموضوعات:\\n- {topics}\\n\"\n",
    "        f\"{topics_knowledge_personal}\\n\\n{topics_knowledge_other}\\n\"\n",
    "        f\"{contextual_usage}\\n\"\n",
    "        f\"تنبيه: الالتزام بما سبق إلزامي في كل إجابة.\"\n",
    "    )\n",
    "\n",
    "PERSONA_PREAMBLE = build_persona_preamble(character)\n",
    "\n",
    "\n",
    "def reformulate_query(query):\n",
    "    model=\"gpt-4o\"\n",
    "    system_prompt = (\n",
    "        \"أنت مساعد متخصص في إعادة صياغة الأسئلة بطريقة مهنية ضمن نظام استرجاع المعلومات (RAG).\"\n",
    "        \"إذا كان السؤال مكتوبًا باللهجة اللبنانية بأحرف إنجليزية، ترجم السؤال إلى العربية الفصحى بأكثر طريقة احترافية ممكنة مع الحفاظ على المهنة في التعبير.\"\n",
    "        \"ابدأ داخليًا بخطة مختصرة من ٣ إلى ٥ خطوات مفاهيمية لمعالجة كل مرحلة من مراحل السؤال، ولا تضمن هذه الخطة في النتيجة النهائية. \"\n",
    "        \"أعد كتابة السؤال بنفس الصيغة المستخدمة من قبل المتكلم (لا تغير الضمائر أو وجهة النظر)، ولا تضف أو تحذف أي معنى جديد.\"\n",
    "        \"إذا كان السؤال واضحًا ومباشرًا، أعِد عرضه كما هو مع تحسين طفيف للأسلوب فقط.\"\n",
    "        \"الهدف هو جعل السؤال أوضح وأكثر رسمية دون تغيير معناه أو صيغة المتكلم. \"\n",
    "        \"بعد تعديل كل سؤال، تحقق داخليًا في جملة أو جملتين أن التعديل حقق الوضوح والاحترافية دون تغيير الجوهر. \"\n",
    "        \"اكتب فقط الصيغة النهائية للسؤال دون شرح أو خطوات.\"\n",
    "        \"الإخراج دائمًا عبارة عن السؤال النهائي المعاد صياغته فقط (جملة واحدة أو أكثر باللغة العربية الفصحى). لا تشرح أو تدرج أي تفاصيل عن العملية أو القوائم المنفذة داخليًا — الناتج النهائي هو السؤال فقط.\"\n",
    "        )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def generate_answer_with_history(user_id, query, retrieved_chunks, formatted_history: str):\n",
    "    model = \"gpt-4o\"\n",
    "    context = \"\\n\\n\".join([c[\"chunk\"] for c in retrieved_chunks])\n",
    "    query_reformulated = reformulate_query(query)\n",
    "\n",
    "    # Hierarchical system prompt with strict logic and persona embedding\n",
    "    system_prompt = f'''\n",
    "أنت مساعد افتراضي تمثل شخصية السيد هاشم صفي الدين، خطيب ديني متمرس، وتلتزم تمامًا بالأسلوب، النبرة، الإيقاع، والمعجم المحدد في التعليمات التالية:\n",
    "{PERSONA_PREAMBLE}\n",
    "\n",
    "قواعد التصرف:\n",
    "1. إذا كان السؤال من فئة \"الأسئلة العامة أو التحية\" (مثل: \"السلام عليكم\"، \"كيف حالك؟\"، \"ماذا تفعل؟\"، \"صباح الخير\"، \"مساء الخير\"، \"من أنت؟\"، \"من المتكلم؟\"، \"ما هو دورك؟\"، \"ما اسمك؟\"، \"ما وظيفتك؟\"، \"ما هدفك؟\"، \"ما هي شخصيتك؟\"، \"ما هي مهمتك؟\"، \"ما هي قدراتك؟\"، \"ما هي حدودك؟\"، \"ما هي رسالتك؟\"، \"ما هي قيمك؟\"، \"ما هي مبادئك؟\"، \"ما هي رؤيتك؟\"، \"ما هي أهدافك؟\"، \"ما هي اهتماماتك؟\"، \"ما هي خلفيتك؟\"، \"ما هي خبرتك؟\"، \"ما هي معرفتك؟\"، \"ما هي مصادر معرفتك؟\"، \"ما هي حدود معرفتك؟\"، \"ما هي إمكانياتك؟\"، \"ما هي حدود إمكانياتك؟\"، \"ما هي حدود قدراتك؟\"، \"ما هي حدود معلوماتك؟\"، \"ما هي حدود خبرتك؟\"، \"ما هي حدود رؤيتك؟\"، \"ما هي حدود رسالتك؟\"، \"ما هي حدود أهدافك؟\"، \"ما هي حدود اهتماماتك؟\"، \"ما هي حدود خلفيتك؟\")، فأجب مباشرة باستخدام الأسلوب المحدد أعلاه دون الحاجة لاسترجاع مقاطع نصية.\n",
    "\n",
    "2. في جميع الحالات الأخرى:\n",
    "   - إذا لم يتم استرجاع أي مقاطع نصية (retrieved chunks)، لا تجب على السؤال مطلقًا وصرّح بوضوح أنك لا تملك معلومات كافية للإجابة.\n",
    "   - يجب أن تستند كل معلومة في إجابتك حصريًا إلى المقاطع المسترجعة.\n",
    "   - لا تبتكر أو تخمّن أو تضف أي معلومة غير موجودة في المقاطع المسترجعة.\n",
    "   - إذا لم تجد الجواب في المقاطع المسترجعة، اعترف بحدود معرفتك ولا تحاول الإجابة.\n",
    "\n",
    "منهجية الإجابة:\n",
    "1. استخرج أولاً المحتوى والمعلومات من المقاطع النصية المسترجعة فقط.\n",
    "2. بعد ذلك، صغ هذا المحتوى بأسلوب السيد هاشم صفي الدين، مع الالتزام بالنبرة، الإيقاع، المعجم، وقوالب الخطاب المحددة أعلاه.\n",
    "3. لا تذكر أو تشير إلى عملية الاسترجاع أو قاعدة المعرفة أو أي تفاصيل تقنية في إجابتك.\n",
    "4. أجب دائمًا باللغة العربية الفصحى، مع إمكانية إدخال لمسات خفيفة من العامية اللبنانية إذا كان ذلك مناسبًا للأسلوب.\n",
    "5. تحدث باحترام عن الشخصيات الدينية واذكر الألقاب المناسبة.\n",
    "6. لا تشرح أو تدرج أي تفاصيل عن الطريقة أو القوائم المفاهيمية المنفذة داخليًا، اكتفِ بكتابة الجواب فقط.\n",
    "'''\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"system\", \"content\": f\"\\n\\nالرسائل السابقة:\\n{formatted_history}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"السياق:\\n\\n{context}\\n\\nالسؤال: {query_reformulated}\"},\n",
    "    ]\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    answer_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Build unique citations list from retrieved chunks' metadata\n",
    "    citations = []\n",
    "    if retrieved_chunks:\n",
    "        seen = set()\n",
    "        for item in retrieved_chunks:\n",
    "            md = item.get(\"metadata\", {})\n",
    "            title = (md.get(\"title\") or \"\").strip()\n",
    "            source = (md.get(\"source\") or \"\").strip()\n",
    "            key = (title, source)\n",
    "            if (title or source) and key not in seen:\n",
    "                seen.add(key)\n",
    "                if title and source:\n",
    "                    citations.append(f\"- {title} — {source}\")\n",
    "                elif title:\n",
    "                    citations.append(f\"- {title}\")\n",
    "                else:\n",
    "                    citations.append(f\"- {source}\")\n",
    "\n",
    "    if citations:\n",
    "        answer_text = f\"{answer_text}\\n\\nالمصادر:\\n\" + \"\\n\".join(citations)\n",
    "\n",
    "    return answer_text\n",
    "\n",
    "\n",
    "# Handlers\n",
    "async def clear_history(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    user_id = update.effective_user.id\n",
    "    history.clear(user_id)\n",
    "    await update.message.reply_text(\"تم مسح تاريخ المحادثة.\")\n",
    "\n",
    "\n",
    "async def handle_message(type, update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    user_id = update.effective_user.id\n",
    "    try:\n",
    "        if type == \"voice\":\n",
    "            voice_file = await update.message.voice.get_file()\n",
    "            audio_bytes = BytesIO()\n",
    "            await voice_file.download_to_memory(out=audio_bytes)\n",
    "            audio_bytes.seek(0)\n",
    "\n",
    "            transcription = elevenlabs.speech_to_text.convert(\n",
    "                file=audio_bytes,\n",
    "                model_id=\"scribe_v1\",\n",
    "                tag_audio_events=True,\n",
    "                language_code=\"ara\",\n",
    "                diarize=True,\n",
    "            )\n",
    "            user_input = transcription.text\n",
    "\n",
    "        elif type == \"text\":\n",
    "            user_input = update.message.text\n",
    "        else:\n",
    "            await update.message.reply_text(\"الرجاء إرسال رسالة صوتية أو نصية فقط، الصيغة المُرسلة غير مدعومة.\")\n",
    "            return\n",
    "\n",
    "        history.add_message(user_id, \"user\", user_input)\n",
    "        prior = history.get_recent_history(user_id, max_messages=20)\n",
    "        formatted = format_history_for_prompt(prior)\n",
    "\n",
    "        query = reformulate_query(user_input)\n",
    "        retrieved_chunks = search_index(query)\n",
    "        answer = generate_answer_with_history(user_id, user_input, retrieved_chunks=retrieved_chunks, formatted_history=formatted)\n",
    "        history.add_message(user_id, \"system\", answer)\n",
    "\n",
    "        await update.message.reply_text(answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        await update.message.reply_text(f\"حدث خطآ، حاول مجددا. {e}\")\n",
    "        # delete e later\n",
    "\n",
    "\n",
    "# Build Bot\n",
    "app = Application.builder().token(telegram_token).build()\n",
    "# Commands\n",
    "app.add_handler(CommandHandler(\"clear\", clear_history))\n",
    "# Messages\n",
    "app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, partial(handle_message, \"text\")))\n",
    "app.add_handler(MessageHandler(filters.VOICE & ~filters.COMMAND, partial(handle_message, \"voice\")))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Bot is running\")\n",
    "    app.run_polling()\n",
    "    print(\"Bot has stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

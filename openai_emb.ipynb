{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 100 chunks from /Users/mohamad/Documents/GitHub/Personalized-RAG-Chatbot/chunks.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "chunks = []\n",
    "\n",
    "folder = r\"/Users/mohamad/Documents/GitHub/Personalized-RAG-Chatbot/chunks.json\"\n",
    "\n",
    "with open(folder, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    if isinstance(data, list):\n",
    "        chunks.extend(data)\n",
    "    elif isinstance(data, dict):\n",
    "        chunks.append(data)\n",
    "    else:\n",
    "        print(f\"Skipping {folder} as it is not a list or dictionary.\")\n",
    "\n",
    "print(f\"✅ Loaded {len(chunks)} chunks from {folder}\")\n",
    "\n",
    "texts = [c[\"content\"] for c in chunks]\n",
    "metadata = [\n",
    "    {\n",
    "        \"id\": c[\"id\"],\n",
    "        \"title\": c[\"title\"],\n",
    "        \"source\": c[\"source\"],\n",
    "        \"text\": c[\"content\"]\n",
    "    }\n",
    "    for c in chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "texts = [c[\"content\"] for c in chunks]\n",
    "embeddings = []\n",
    "\n",
    "for text in tqdm(texts, desc=\"Embedding texts\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    emb = np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "    emb /= np.linalg.norm(emb)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index created and saved with 100 vectors.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import os\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "os.makedirs(\"storage\", exist_ok=True)\n",
    "faiss.write_index(index, \"storage/openai_index.faiss\")\n",
    "\n",
    "print(f\"✅ FAISS index created and saved with {index.ntotal} vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    embedding = response.data[0].embedding\n",
    "    return np.array(embedding, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "os.makedirs(\"storage\", exist_ok=True)\n",
    "\n",
    "metadata = [\n",
    "    {\n",
    "        \"id\": i,\n",
    "        \"title\": c.get(\"title\", \"\"),\n",
    "        \"source\": c.get(\"source\", \"\"),\n",
    "        \"content\": c[\"content\"]\n",
    "    }\n",
    "    for i, c in enumerate(chunks)\n",
    "]\n",
    "\n",
    "\n",
    "with open(\"storage/chunks_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top relevant chunks:\n",
      "\n",
      "Result 1 (score: 0.4982):\n",
      "ثم لا يتغير ولا يتبدل ولا يتأثر ولا ينفعل. ما هذا قلب محجوب وهذا قلب لا يشعر هذه لا يشعر بهذا الدعاء وأهميته بل لا يستطعن طعمه ولذته. لان لذة الدعاء تقربك من الله سبحانه وتعالى. ولذا ورد عندنا ان في بعض بعض من يدعو. بعض الروايات ورد عندنا أن من يدعو الله عز وجل هو يعرف في قرارة نفسه وفي خاتمة دعائه أن دعاؤه وصل أو لم يصل. كيف؟ إذا شعر في قلبه انفعالا. وفي بعض الروايات تتحدث عن قطرة دمع تنزل من عينيه. هذه إشارة على استجابة الدعاء وعلى وصول الدعاء لأن هناك انفعالا ولأن هناك تأثرا على أي حال.\n",
      "{'id': 56, 'title': 'عاشوراء 2016 - 1438 هجري » كلمة السيد هاشم صفي الدين في الليلة السادسة من شهر محرم الحرام.', 'source': 'موقع السيد هاشم صفي الدين'}\n",
      "\n",
      "Result 2 (score: 0.4262):\n",
      "حياتنا هوى. حياتنا شهوة. حياتنا نزوة حياتنا. رغبة حينئذ نحن محجوبون عن الله عز وجل. حينما يغلبنا الهوى. الدعاء لا ينفع. التوسل لا ينفع. الرجوع إلى الله بعد إذن لا ينفع. لا بد أن تقلع عن هذا الهوى حتى يستقيم الأمر. وفي الحديث القدسي إن أدني يقول الله تعالى إن أدني ما أصنع بالعبد إذا آثر شهوته على طاعتي أن أحرمه لذيذ مناجاة.شوفوا اللي بيقعد بالدعاء بنحي ليلة القدر مع سيد الشهداء في هذا المكان المبارك وفي كل المساجد وكل المساجد مباركة. انسان يحي ليلة القدر بادعية طويلة وردت عن اهل البيت عليهم السلام.\n",
      "{'id': 55, 'title': 'عاشوراء 2016 - 1438 هجري » كلمة السيد هاشم صفي الدين في الليلة السادسة من شهر محرم الحرام.', 'source': 'موقع السيد هاشم صفي الدين'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "import openai\n",
    "\n",
    "index = faiss.read_index(\"storage/openai_index.faiss\")\n",
    "\n",
    "with open(\"storage/chunks_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "def search_index(query, k=5, min_score=0.4):\n",
    "    query_vector = get_embedding(query).reshape(1, -1)\n",
    "    query_vector /= np.linalg.norm(query_vector)\n",
    "\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if dist >= min_score:\n",
    "            chunk_data = metadata[idx]\n",
    "            results.append({\n",
    "                \"score\": float(dist),\n",
    "                \"chunk\": chunk_data[\"content\"],\n",
    "                \"metadata\": {\n",
    "                    \"id\": chunk_data[\"id\"],\n",
    "                    \"title\": chunk_data[\"title\"],\n",
    "                    \"source\": chunk_data[\"source\"]\n",
    "                }\n",
    "        })\n",
    "    return results\n",
    "\n",
    "query = \"ما هي العلامات التي تدل على استجابة دعاء الإنسان؟\"\n",
    "query = \"كيف يمكن للإنسان أن يعرف أن دعاءه قد استجيب؟\"\n",
    "results = search_index(query, k=6)\n",
    "\n",
    "print(\"Top relevant chunks:\")\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i} (score: {res['score']:.4f}):\")\n",
    "    print(res[\"chunk\"])\n",
    "    print(res[\"metadata\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_query(query):\n",
    "    model=\"gpt-5-mini\"\n",
    "    system_prompt = (\n",
    "        \"أنت مساعد متخصص في إعادة صياغة الأسئلة بطريقة مهنية ضمن نظام استرجاع المعلومات (RAG system).\"\n",
    "        \"إذا كان السؤال مكتوبًا باللهجة اللبنانية بأحرف إنجليزية، ترجم السؤال إلى العربية الفصحى بأكثر طريقة احترافية ممكنة مع الحفاظ على الاحترافية في التعبير، مع الأخذ بعين الاعتبار سياق المحادثة وسيرها السابق للتحقق من توافق السؤال مع مجرى الحوار.\"\n",
    "        \"ابدأ بخطة مختصرة (Checklist) من 3-5 خطوات مفاهيمية لكل مرحلة تعالج فيها السؤال.\"\n",
    "        \"أعد كتابة السؤال بنفس الصيغة المستخدمة من قبل المتكلم (لا تغيّر الضمائر أو وجهة النظر).\"\n",
    "        \"لا تضف أو تحذف أي معنى جديد.\"\n",
    "        \"إذا كان السؤال واضحًا ومباشرًا، أعد عرضه كما هو مع تحسين طفيف للأسلوب فقط.\"\n",
    "        \"الهدف هو جعل السؤال أوضح وأكثر رسمية دون تغيير معناه أو صيغة المتكلم.\"\n",
    "        \"بعد تعديل كل سؤال، تحقق في جملة أو جملتين من أن التعديل حقق الوضوح والاحترافية دون تغيير الجوهر، ثم انتقل للسؤال التالي.\"\n",
    "        \"إذا لم يتوافق السؤال مع ما قبل، أعد صياغته مرة ثانية وتحقق مرة أخرى من توافقه مع مجرى الحوار.\"\n",
    "        \"اكتب فقط الصيغة النهائية للسؤال من دون الشرح والخطوات.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate_answer(query, retrieved_chunks, model=\"gpt-4o-mini\"):\n",
    "\n",
    "    context = \"\\n\\n\".join([c[\"chunk\"] for c in retrieved_chunks])\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"أنت مساعد يجيب على انه السيد هاشم صفي الدين.\"\n",
    "                \"اعتمد في اجابتك على النصوص المتوفرة في السياق فقط.\"\n",
    "                \"إذا لم يكن الجواب واضحا وكاملا في السياق، قل أنك لا تعرف. \"\n",
    "                \"تكلم باحترام عن الشخصيات الشيعية, مع ذكر الألقاب المناسبة.\"\n",
    "                \"أجب دائمًا باللغة العربية الفصحى الواضحة.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"السياق:\\n{context}\\n\\nالسؤال: {query}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عذرًا، لا أستطيع تقديم رأي حول ما حدث بالأمس، حيث لا تتوفر لدي معلومات كافية عن الحدث المعني.\n"
     ]
    }
   ],
   "source": [
    "query = \"ما هي العلامات التي تدل على استجابة دعاء الإنسان؟\"\n",
    "query = \"كيف يمكن للإنسان أن يعرف أن دعاءه قد استجيب؟\"\n",
    "query = \"ما هو رأيك فيما حدث بالأمس؟\"\n",
    "\n",
    "results = search_index(query, k=3)\n",
    "\n",
    "retrieved_chunks = results\n",
    "answer = generate_answer(query, retrieved_chunks)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Reformulated: قائمة التحقق (Checklist):\n",
      "1. تحديد اللهجة والنية: ترجمة العبارة من اللهجة اللبنانية المكتوبة باللاتينية إلى العربية الفصحى.\n",
      "2. الحفاظ على صيغة المتكلم والضمائر: الإبقاء على صيغة الأمر للمخاطبين كما وردت.\n",
      "3. ضبط الأسلوب ليكون رسميًا وواضحًا دون تغيير المعنى.\n",
      "4. مراجعة للتأكد من الوضوح والاحترافية.\n",
      "\n",
      "اكتبوا مُرقَّمًا\n"
     ]
    }
   ],
   "source": [
    "history = \"{'role': 'system', 'content': '\\n\\nالرسائل السابقة:\\nuser: aan shu btaaref tehke'}, {'role': 'user', 'content': 'السياق:\\n\\n\\n\\nالسؤال: عن ماذا تستطيع التحدث؟'}]\"\n",
    "query = \"ktebun mra2amen\"\n",
    "# query =  \"سلام\"\n",
    "\n",
    "refined_query = reformulate_query(query)\n",
    "print(\"🔄 Reformulated:\", refined_query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
